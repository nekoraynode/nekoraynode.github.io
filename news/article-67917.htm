<!DOCTYPE html>
<html xml:lang="zh-CN" lang="zh-CN">

<head>
        <link rel="canonical" href="https://nekoraynode.github.io/news/article-67917.htm" />
    <meta charset="utf-8">
    <meta content="width=device-width, initial-scale=1.0" name="viewport">
    <title>底层实现dropout——【torch学习笔记】</title>
        <meta name="description" content="dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。注意是暂时，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。" />
        <link rel="icon" href="/assets/website/img/nekoraynode/favicon.ico" type="image/x-icon"/>

    <meta name="author" content="NekoRayNode节点订阅站">
    <meta property="og:type" content="article" />
    <meta property="og:url" content="https://nekoraynode.github.io/news/article-67917.htm" />
    <meta property="og:site_name" content="NekoRayNode节点订阅站" />
    <meta property="og:title" content="底层实现dropout——【torch学习笔记】" />
    <meta property="og:image" content="https://nekoraynode.github.io/uploads/20240626/e38ecf14d34b9dd7014dc90ae963cd0e.webp" />
        <meta property="og:release_date" content="2025-03-05T11:12:27" />
    <meta property="og:updated_time" content="2025-03-05T11:12:27" />
        <meta property="og:description" content="dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。注意是暂时，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。" />
        
    <meta name="applicable-device" content="pc,mobile" />
    <meta name="renderer" content="webkit" />
    <meta name="force-rendering" content="webkit" />
    <meta http-equiv="Cache-Control" content="no-transform" />
    <meta name="robots" content="max-image-preview:large" />
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="apple-mobile-web-app-title" content="底层实现dropout——【torch学习笔记】">
    <meta name="format-detection" content="telephone=no">

    <link rel="dns-prefetch" href="https:/www.googletagmanager.com">
    <link rel="dns-prefetch" href="https://www.googleadservices.com">
    <link rel="dns-prefetch" href="https://www.google-analytics.com">
    <link rel="dns-prefetch" href="https://pagead2.googlesyndication.com">
    <link rel="dns-prefetch" href="https://cm.g.doubleclick.net">
    
    <!-- Google Web Fonts -->
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Plus+Jakarta+Sans:wght@500;600;700&family=Rubik:wght@400;500&display=swap" rel="stylesheet">
    <!-- Icon Font Stylesheet -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.4/css/all.css" />
    <!-- Libraries Stylesheet -->
    <link href="/assets/website/js/frontend/nekoraynode/lightbox/css/lightbox.min.css" rel="stylesheet">
    <!-- Customized Bootstrap Stylesheet -->
    <link href="/assets/website/css/nekoraynode/bootstrap.min.css" rel="stylesheet">
    <!-- Template Stylesheet -->
    <link href="/assets/website/css/nekoraynode/style.css" rel="stylesheet">
    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-E6TD6QHWW6"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-E6TD6QHWW6');
</script>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3332997411212854"
     crossorigin="anonymous"></script>
</head>

<body data-page="detail">
    <!-- Spinner Start -->
    <div id="spinner" class="show bg-white position-fixed translate-middle w-100 vh-100 top-50 start-50 d-flex align-items-center justify-content-center">
        <div class="spinner-border text-primary" style="width: 3rem; height: 3rem;" role="status">
            <span class="sr-only">Loading...</span>
        </div>
    </div>
    <!-- Spinner End -->
    <!-- Navbar & Hero Start -->
    <div class="container-fluid p-0">
                <nav class="navbar navbar-expand-lg fixed-top navbar-light px-4 px-lg-5 py-3 py-lg-0">
            <a href="/" class="navbar-brand p-0">
                <a href="/">
                                <span class="display-6 text-primary m-0">
                    <i class="fas fa-envelope me-3"></i>NekoRay Node                </span>
                                </a>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse">
                <span class="fa fa-bars"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarCollapse">
                <div class="navbar-nav ms-auto py-0">
                                        <a href="/" class="nav-item nav-link">首页</a>
                                        <a href="/free-nodes/" class="nav-item nav-link">免费节点</a>
                                        <a href="/paid-subscribe/" class="nav-item nav-link">推荐机场</a>
                                        <a href="/client.htm" class="nav-item nav-link">客户端</a>
                                        <a href="/news/" class="nav-item nav-link">新闻资讯</a>
                                    </div>
            </div>
        </nav>
    </div>
    <!-- Navbar & Hero End -->
    <!-- Header Start -->
    <div class="container-fluid bg-breadcrumb">
        <ul class="breadcrumb-animation">
            <li></li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
            <li></li>
        </ul>
        <div class="container text-center py-5" style="max-width: 900px;">
            <h3 class="display-3 mb-4 wow fadeInDown" data-wow-delay="0.1s">底层实现dropout——【torch学习笔记】</h1>
                <ol class="breadcrumb justify-content-center mb-0 wow fadeInDown" data-wow-delay="0.3s">
                    <li class="breadcrumb-item"><a href="/">首页</a></li>
                    <li class="breadcrumb-item"><a href="/news/">新闻资讯</a></li>
                    <li class="breadcrumb-item active text-primary">正文</li>
                </ol>
        </div>
    </div>
    <!-- Header End -->
    <!-- About Start -->
    <div class="container-fluid py-5">
        <div class="container py-5">
            <div class="row">
                <div class="col-md-9">
                                    <input type="hidden" id="share-website-info" data-name="" data-url="">
                  				  				  				<div id="content_views" class="markdown_views prism-github-gist"> </h1> <p>dropout是指在深度学习网络的训练过程中，对于神经网络单元，按照一定的概率将其暂时从网络中丢弃。注意是暂时，对于随机梯度下降来说，由于是随机丢弃，故而每一个mini-batch都在训练不同的网络。</p> <p>引用翻译：《动手学深度学习》</p> <h2> <a id="_7" rel="nofollow"></a>一、重新审视过度拟合</h2> <p>鉴于特征比例子多得多，线性模型可以过度拟合。但是，当例子比特征多时，我们通常可以指望线性模型不会过度拟合。不幸的是，线性模型归纳的可靠性是有代价的。线性模型不能考虑到特征之间的相互作用。对于每个特征，线性模型必须分配一个正的或负的权重。它们缺乏考虑上下文的灵活性。</p> <p>在更正式的文本中，你会看到这种可概括性和灵活性之间的基本矛盾被讨论为偏倚-变异权衡。线性模型有很高的偏差（它们只能代表一小类函数），但方差很低（它们在数据的不同随机样本中给出相似的结果）。</p> <p>深度神经网络将我们带到了偏差-方差光谱的另一端。神经网络之所以如此灵活，是因为它们并不局限于单独观察每个特征。相反，它们可以学习各组特征之间的相互作用。例如，它们可以推断出 "尼日利亚 "和 "西联汇款 "同时出现在一封电子邮件中表明是垃圾邮件，但没有 "西联汇款 "的 "尼日利亚 "则不是。</p> <p>即使我们只有少量的特征，深度神经网络也有能力进行过度拟合。2017年，一组研究人员提出了一个现在众所周知的关于神经网络难以置信的灵活性的演示。他们向一个神经网络展示了随机标记的图像（没有真正的模式将输入和输出联系起来），并发现由SGD优化的神经网络可以完美地标记训练集中的每一张图像。</p> <p>考虑一下这意味着什么。如果标签是统一随机分配的，并且有10个类别，那么没有一个分类器能在保持数据上获得优于10%的准确性。然而，即使在这种情况下，当没有真正的模式可以学习时，神经网络也能完美地适应训练标签。</p> <h2> <a id="_19" rel="nofollow"></a>二、通过扰动的鲁棒性</h2> <p>让我们简单思考一下我们对一个好的统计模型的期望。我们希望它在未见过的测试数据上表现良好。我们可以通过询问什么是 "简单 "的模型来实现这一目标？简洁性可以以少量维度的形式出现，这就是我们在讨论用单项式基函数拟合模型时的做法。简洁性也可以以基函数的小规范的形式出现。这使我们想到了权重衰减（ℓ2正则化）。然而，我们可以施加的第三个简单性概念是，函数在输入的小变化下应该是稳健的。例如，当我们对图像进行分类时，我们希望在像素上添加一些随机噪声应该是无害的。</p> <p>1995年，Christopher Bishop正式提出了这个想法的一种形式，他证明了用输入噪声进行训练等同于Tikhonov正则化。换句话说，他在要求一个函数是平滑的（因而也是简单的）（我们在关于权重衰减的章节中讨论过）与要求它对输入的扰动有弹性之间建立了明确的数学联系。</p> <p>然后在2014年，Srivastava等人提出了一个聪明的想法，即如何将Bishop的想法也应用到网络的内部层。也就是说，他们提议在训练过程中，在计算后续层之前向网络的每一层注入噪声。他们意识到，在训练有很多层的深层网络时，仅仅在输入-输出映射上强制执行平滑性会忽略网络内部发生的事情。他们提出的想法被称为dropout，它现在是一种标准技术，被广泛用于训练神经网络。在整个训练过程中，在每一次迭代中，dropout正则化仅仅包括在计算下一层之前将每一层中的一些节点清零（通常是50%）。</p> <p>那么关键的挑战是如何在不引入不适当的统计偏差的情况下注入这种噪音。换句话说，我们希望在训练过程中对每一层的输入进行扰动，使该层的预期值等于我们没有引入任何噪声时的值。<br /> 在Bishop的例子中，当我们在线性模型中加入高斯噪声时，这很简单。在每个训练迭代中，只需向输入的????∼(0,????2)添加从均值为零的分布中采样的噪声，产生一个扰动点 ????′=????+????。在期望值中，????[????′]=????。</p> <p>在Dropout正则化的情况下，我们可以通过对未被Dropout的节点的比例进行归一化，来对每一层进行debias。换句话说，掉线概率为????的掉线被应用如下。</p> <p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"></p> <p>              h</p> <p>              ′</p> <p>             =</p> <p>              {</p> <p>                  0</p> <p>                   &nbsp;with&nbsp;probability&nbsp;</p> <p>                   p</p> <p>                   h</p> <p>                    1</p> <p>                    −</p> <p>                    p</p> <p>                  &nbsp;otherwise</p> <p>         \begin{aligned} h' = \begin{cases} 0 &amp; \text{ with probability } p \\ \frac{h}{1-p} &amp; \text{ otherwise} \end{cases} \end{aligned} </p> <p>     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 3.30003em; vertical-align: -1.40002em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.90002em;"><span class="" style="top: -3.90002em;"><span class="pstrut" style="height: 3.75em;"></span><span class="mord"><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height: 0.801892em;"><span class="" style="top: -3.113em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="minner"><span class="mopen delimcenter" style="top: 0em;"><span class="delimsizing size4">{<!-- --></span></span><span class="mord"><span class="mtable"><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.71455em;"><span class="" style="top: -3.71455em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord">0</span></span></span><span class="" style="top: -2.27455em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.880108em;"><span class="" style="top: -2.655em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mbin mtight">−</span><span class="mord mathdefault mtight">p</span></span></span></span><span class="" style="top: -3.23em;"><span class="pstrut" style="height: 3em;"></span><span class="frac-line" style="border-bottom-width: 0.04em;"></span></span><span class="" style="top: -3.394em;"><span class="pstrut" style="height: 3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">h</span></span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height: 0.481108em;"><span class=""></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height: 1.21455em;"><span class=""></span></span></span></span></span><span class="arraycolsep" style="width: 1em;"></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 1.71455em;"><span class="" style="top: -3.71455em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord text"><span class="mord">&nbsp;with&nbsp;probability&nbsp;</span></span><span class="mord mathdefault">p</span></span></span><span class="" style="top: -2.27455em;"><span class="pstrut" style="height: 3.008em;"></span><span class="mord"><span class="mord text"><span class="mord">&nbsp;otherwise</span></span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height: 1.21455em;"><span class=""></span></span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height: 1.40002em;"><span class=""></span></span></span></span></span></span></span></span></span></span></span></span></p> <p>根据设计，期望值保持不变，即????[ℎ′]=ℎ。中间激活ℎ被一个具有匹配期望值的随机变量ℎ′所取代。Dropout "这一名称源于这样一个概念，即一些神经元为了计算最终结果而 “Dropout”。在训练过程中，我们用随机变量取代中间激活。</p> <h2> <a id="dropout_44" rel="nofollow"></a>三、实践中的dropout</h2> <p>回顾多层感知器（:numref:chapter_mlp），有一个隐藏层和5个隐藏单元。它的结构是这样的</p> <p><span class="katex--display"><span class="katex-display"><span class="katex"><span class="katex-mathml"></p> <p>            h</p> <p>             =</p> <p>             σ</p> <p>             (</p> <p>              W</p> <p>              1</p> <p>             x</p> <p>             +</p> <p>              b</p> <p>              1</p> <p>             )</p> <p>            o</p> <p>             =</p> <p>              W</p> <p>              2</p> <p>             h</p> <p>             +</p> <p>              b</p> <p>              2</p> <p>             y</p> <p>             ^</p> <p>             =</p> <p>              s</p> <p>              o</p> <p>              f</p> <p>              t</p> <p>              m</p> <p>              a</p> <p>              x</p> <p>             (</p> <p>             o</p> <p>             )</p> <p>         \begin{aligned} h &amp; = \sigma(W_1 x + b_1) \\ o &amp; = W_2 h + b_2 \\ \hat{y} &amp; = \mathrm{softmax}(o) \end{aligned} </p> <p>     </span><span class="katex-html"><span class="base"><span class="strut" style="height: 4.5em; vertical-align: -2em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.5em;"><span class="" style="top: -4.66em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault">h</span></span></span><span class="" style="top: -3.16em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault">o</span></span></span><span class="" style="top: -1.66em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.69444em;"><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.03588em;">y</span></span></span><span class="" style="top: -3em;"><span class="pstrut" style="height: 3em;"></span><span class="accent-body" style="left: -0.19444em;">^</span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height: 0.19444em;"><span class=""></span></span></span></span></span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height: 2em;"><span class=""></span></span></span></span></span><span class="col-align-l"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 2.5em;"><span class="" style="top: -4.66em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mord mathdefault" style="margin-right: 0.03588em;">σ</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.13889em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord mathdefault">x</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mclose">)</span></span></span><span class="" style="top: -3.16em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right: 0.13889em;">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: -0.13889em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span><span class="mord mathdefault">h</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right: 0.222222em;"></span><span class="mord"><span class="mord mathdefault">b</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height: 0.301108em;"><span class="" style="top: -2.55em; margin-left: 0em; margin-right: 0.05em;"><span class="pstrut" style="height: 2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height: 0.15em;"><span class=""></span></span></span></span></span></span></span></span><span class="" style="top: -1.66em;"><span class="pstrut" style="height: 3em;"></span><span class="mord"><span class="mord"></span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right: 0.277778em;"></span><span class="mord"><span class="mord mathrm">s</span><span class="mord mathrm">o</span><span class="mord mathrm" style="margin-right: 0.07778em;">f</span><span class="mord mathrm">t</span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span><span class="mopen">(</span><span class="mord mathdefault">o</span><span class="mclose">)</span></span></span></span><span class="vlist-s"></span></span><span class="vlist-r"><span class="vlist" style="height: 2em;"><span class=""></span></span></span></span></span></span></span></span></span></span></span></span></p> <p>当我们对隐藏层应用dropout时，我们基本上是以概率????移除每个隐藏单元，（即把它们的输出设置为0）。我们可以把这个结果看作是一个只包含原始神经元子集的网络。在下面的图片中，ℎ2和ℎ5被删除。因此，????的计算不再依赖于ℎ2和ℎ5，它们各自的梯度也在执行Backprop时消失。这样一来，输出层的计算就不能过度依赖ℎ1,…,ℎ5中的任何一个元素。直观地说，深度学习的研究者们经常这样解释这个inutition：我们不希望网络的输出过于不稳定地依赖于通过网络的确切激活途径。dropout技术的原作者将他们的直觉描述为防止特征检测器的共同适应的一种努力。</p> <pre><code class="prism language-python"><span class="token keyword">from</span> IPython<span class="token punctuation">.</span>display <span class="token keyword">import</span> SVG SVG<span class="token punctuation">(</span>filename <span class="token operator">=</span> <span class="token string">'../img/dropout2.svg'</span><span class="token punctuation">)</span> </code></pre> <p><img decoding="async" src="http://img.555519.xyz/uploads/20230217/d211ae26a24966e1e22435a0bc2d3807.jpg" alt="底层实现dropout——【torch学习笔记】"></p> <p>在测试时，我们通常不使用辍学。然而，我们注意到有一些例外情况：一些研究人员在测试时使用辍学作为估计神经网络预测的信心的启发式评估：如果预测在许多不同的辍学掩码中都一致，那么我们可以说网络更有信心。现在，我们将把不确定性估计的高级话题推迟到以后的章节和卷中。</p> <h2> <a id="dropout_77" rel="nofollow"></a>四、从零开始实施dropout</h2> <p>为了实现单层的剔除功能，我们必须从伯努利（二进制）随机变量中抽取尽可能多的样本，因为我们的层有维数，其中随机变量的值为1（保留），概率为1-????，0（剔除），概率为????。一个简单的实现方法是首先从均匀分布????[0,1]中抽取样本。然后我们可以保留那些相应样本大于????的节点，放弃其余的节点。</p> <p>在下面的代码中，我们实现了一个dropout函数，该函数以drop_prob的概率丢掉了张量输入X中的元素，如上所述重新调整了剩余部分的比例（将幸存者除以1.0-drop_prob）。</p> <pre><code class="prism language-python"><span class="token keyword">import</span> torch <span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn <span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F <span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim  <span class="token keyword">import</span> sys sys<span class="token punctuation">.</span>path<span class="token punctuation">.</span>insert<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'..'</span><span class="token punctuation">)</span> <span class="token keyword">def</span> <span class="token function">load_data_fashion_mnist</span><span class="token punctuation">(</span>batch_size<span class="token punctuation">,</span> resize<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> root<span class="token operator">=</span>os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>join<span class="token punctuation">(</span>         <span class="token string">'~'</span><span class="token punctuation">,</span> <span class="token string">'.pytorch'</span><span class="token punctuation">,</span> <span class="token string">'datasets'</span><span class="token punctuation">,</span> <span class="token string">'fashion-mnist'</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token triple-quoted-string string">"""下载数据集并存入内存."""</span>     root <span class="token operator">=</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>expanduser<span class="token punctuation">(</span>root<span class="token punctuation">)</span>     <span class="token keyword">print</span><span class="token punctuation">(</span>root<span class="token punctuation">)</span>     transformer <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>     <span class="token keyword">if</span> resize<span class="token punctuation">:</span>         transformer <span class="token operator">+=</span> <span class="token punctuation">[</span>transforms<span class="token punctuation">.</span>Resize<span class="token punctuation">(</span>resize<span class="token punctuation">)</span><span class="token punctuation">]</span>     transformer <span class="token operator">+=</span> <span class="token punctuation">[</span>transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>     transformer <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span>transformer<span class="token punctuation">)</span>          <span class="token comment"># 如果存在数据集则不再下载，若没有则下载</span>     mnist_train <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>root<span class="token operator">=</span>root<span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transformer<span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>     mnist_test <span class="token operator">=</span> torchvision<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>FashionMNIST<span class="token punctuation">(</span>root<span class="token operator">=</span>root<span class="token punctuation">,</span> train<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> transform<span class="token operator">=</span>transformer<span class="token punctuation">,</span> download<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>     num_workers <span class="token operator">=</span> <span class="token number">0</span> <span class="token keyword">if</span> sys<span class="token punctuation">.</span>platform<span class="token punctuation">.</span>startswith<span class="token punctuation">(</span><span class="token string">'win32'</span><span class="token punctuation">)</span> <span class="token keyword">else</span> <span class="token number">4</span>          <span class="token comment"># 使用封装好的DataLoader载入即可。</span>     train_iter <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>mnist_train<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span>num_workers<span class="token punctuation">)</span>     test_iter <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>mnist_test<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> num_workers<span class="token operator">=</span>num_workers<span class="token punctuation">)</span>     <span class="token keyword">return</span> mnist_train<span class="token punctuation">,</span>mnist_test<span class="token punctuation">,</span>train_iter<span class="token punctuation">,</span> test_iter   </code></pre> <p><strong>dropout的底层实现函数：</strong></p> <pre><code class="prism language-python"><span class="token keyword">def</span> <span class="token function">dropout</span><span class="token punctuation">(</span>X<span class="token punctuation">,</span> drop_prob<span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token keyword">assert</span> <span class="token number">0</span> <span class="token operator">&lt;=</span> drop_prob <span class="token operator">&lt;=</span> <span class="token number">1</span>     <span class="token keyword">if</span> drop_prob <span class="token operator">==</span> <span class="token number">1</span><span class="token punctuation">:</span>  <span class="token comment"># 在这种情况下，所有元素都被剔除</span>         <span class="token keyword">return</span> torch<span class="token punctuation">.</span>zeros_like<span class="token punctuation">(</span>X<span class="token punctuation">)</span>     <span class="token comment"># mask是计算是否保留的0，1值</span>     mask <span class="token operator">=</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span>X<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">.</span>uniform_<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">&gt;</span> drop_prob<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>     <span class="token comment"># 将幸存者除以1.0-drop_prob</span>     <span class="token keyword">return</span> mask <span class="token operator">*</span> X <span class="token operator">/</span> <span class="token punctuation">(</span><span class="token number">1.0</span> <span class="token operator">-</span> drop_prob<span class="token punctuation">)</span> </code></pre> <p>我们可以在几个例子上测试一下dropout函数。在下面几行代码中，我们将我们的输入X通过dropout操作，概率分别为0、0.5和1。</p> <pre><code class="prism language-python"><span class="token comment"># mask = tensor([[0., 0., 0., 0., 0., 1., 1., 0.],</span> <span class="token comment">#        [0., 0., 1., 0., 0., 1., 0., 1.]])</span> X <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span>X<span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span>dropout<span class="token punctuation">(</span>X<span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 相当于将幸存者除(1-0)（即不变）</span> <span class="token keyword">print</span><span class="token punctuation">(</span>dropout<span class="token punctuation">(</span>X<span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 相当于将幸存者除(1-0.5)（即乘以2）</span> <span class="token keyword">print</span><span class="token punctuation">(</span>dropout<span class="token punctuation">(</span>X<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment"># 相当于将幸存者除(1-1)（0）</span> </code></pre> <pre><code>tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],         [ 8.,  9., 10., 11., 12., 13., 14., 15.]]) tensor([[ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.],         [ 8.,  9., 10., 11., 12., 13., 14., 15.]]) tensor([[ 0.,  0.,  4.,  0.,  0., 10.,  0.,  0.],         [16., 18., 20.,  0.,  0.,  0.,  0., 30.]]) tensor([[0., 0., 0., 0., 0., 0., 0., 0.],         [0., 0., 0., 0., 0., 0., 0., 0.]]) </code></pre> <h2> <a id="_153" rel="nofollow"></a>五、定义模型参数</h2> <p>同样，我们可以使用 :numref:chapter_softmax_scratch中介绍的Fashion-MNIST数据集。我们将定义一个有两个隐藏层的多层感知器。这两个隐藏层都有256个输出。</p> <h2> <a id="dropout_157" rel="nofollow"></a>六、应用dropout定义模型</h2> <p>下面定义的模型将全连接层和激活函数ReLU串联起来，对每个激活函数的输出使用dropout。我们可以分别设置每一层的丢弃概率。一般来说，建议在靠近输入层的地方设置较低的丢弃概率。下面我们将第一和第二隐藏层分别设置为0.2和0.5。通过使用:numref:chapter_autograd中描述的is_training函数，我们可以确保dropout只在训练期间有效。</p> <pre><code class="prism language-python">drop_prob1<span class="token punctuation">,</span> drop_prob2 <span class="token operator">=</span> <span class="token number">0.2</span><span class="token punctuation">,</span> <span class="token number">0.5</span>  <span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> num_inputs <span class="token operator">=</span> <span class="token number">784</span><span class="token punctuation">,</span> num_outputs <span class="token operator">=</span> <span class="token number">10</span><span class="token punctuation">,</span> num_hiddens1 <span class="token operator">=</span> <span class="token number">256</span><span class="token punctuation">,</span> num_hiddens2 <span class="token operator">=</span> <span class="token number">256</span><span class="token punctuation">,</span> is_training <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>         <span class="token builtin">super</span><span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>                  self<span class="token punctuation">.</span>num_inputs <span class="token operator">=</span> num_inputs         self<span class="token punctuation">.</span>num_outputs <span class="token operator">=</span> num_outputs         self<span class="token punctuation">.</span>num_hiddens1 <span class="token operator">=</span> num_hiddens1         self<span class="token punctuation">.</span>num_hiddens2 <span class="token operator">=</span> num_hiddens2         self<span class="token punctuation">.</span>is_training <span class="token operator">=</span> is_training                  self<span class="token punctuation">.</span>linear_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_inputs<span class="token punctuation">,</span> num_hiddens1<span class="token punctuation">)</span>         self<span class="token punctuation">.</span>linear_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens1<span class="token punctuation">,</span> num_hiddens2<span class="token punctuation">)</span>         self<span class="token punctuation">.</span>linear_3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>num_hiddens2<span class="token punctuation">,</span> num_outputs<span class="token punctuation">)</span>                  self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>          <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> X<span class="token punctuation">)</span><span class="token punctuation">:</span>         <span class="token comment"># 可以分别设置每一层的丢弃概率</span>         X <span class="token operator">=</span> X<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_inputs<span class="token punctuation">)</span><span class="token punctuation">)</span>         H1 <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>linear_1<span class="token punctuation">(</span>X<span class="token punctuation">)</span><span class="token punctuation">)</span>         <span class="token comment"># 只有在训练阶段使用dropout</span>         <span class="token keyword">if</span> self<span class="token punctuation">.</span>is_training <span class="token operator">==</span> <span class="token boolean">True</span><span class="token punctuation">:</span>             <span class="token comment"># 在第一个全连接层之后添加一个dropout层</span>             <span class="token comment"># 靠近输入层的地方设置较低的丢弃概率</span>             H1 <span class="token operator">=</span> dropout<span class="token punctuation">(</span>H1<span class="token punctuation">,</span> drop_prob1<span class="token punctuation">)</span>         H2 <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>linear_2<span class="token punctuation">(</span>H1<span class="token punctuation">)</span><span class="token punctuation">)</span>         <span class="token keyword">if</span> self<span class="token punctuation">.</span>is_training <span class="token operator">==</span> <span class="token boolean">True</span><span class="token punctuation">:</span>             <span class="token comment"># 在第二个全连接层之后添加一个dropout层</span>             <span class="token comment"># 后面层的地方设置较中等的丢弃概率</span>             H2 <span class="token operator">=</span> dropout<span class="token punctuation">(</span>H2<span class="token punctuation">,</span> drop_prob2<span class="token punctuation">)</span>         out <span class="token operator">=</span> self<span class="token punctuation">.</span>linear_3<span class="token punctuation">(</span>H2<span class="token punctuation">)</span>         <span class="token keyword">return</span> out  net <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span> </code></pre> <pre><code>Net(   (linear_1): Linear(in_features=784, out_features=256, bias=True)   (linear_2): Linear(in_features=256, out_features=256, bias=True)   (linear_3): Linear(in_features=256, out_features=10, bias=True)   (relu): ReLU() ) </code></pre> <h2> <a id="_211" rel="nofollow"></a>七、训练和测试</h2> <p>这与前面描述的多层感知器的训练和测试类似。</p> <pre><code class="prism language-python"><span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn <span class="token keyword">import</span> torch<span class="token punctuation">.</span>optim <span class="token keyword">as</span> optim <span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F <span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable <span class="token keyword">import</span> numpy <span class="token keyword">as</span> np <span class="token keyword">import</span> math <span class="token keyword">import</span> time <span class="token keyword">def</span> <span class="token function">train_ch3</span><span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> criterion<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token triple-quoted-string string">"""使用CPU训练或评估模型."""</span>     optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>net<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>lr<span class="token punctuation">)</span>     <span class="token keyword">for</span> epoch <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>         train_l_sum<span class="token punctuation">,</span> train_acc_sum<span class="token punctuation">,</span> n <span class="token operator">=</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0.0</span><span class="token punctuation">,</span> <span class="token number">0</span>         <span class="token keyword">for</span> X<span class="token punctuation">,</span> y <span class="token keyword">in</span> train_iter<span class="token punctuation">:</span>             <span class="token comment"># 即将梯度初始化为零</span>             optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>             <span class="token comment"># 通过训练的网络net处理该批次数据集X，得到预测的类别输出</span>             y_hat <span class="token operator">=</span> net<span class="token punctuation">(</span>X<span class="token punctuation">)</span>             <span class="token comment"># 通过预测的类别 ，结合真实类别，通过交叉熵计算损失函数</span>             loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> y<span class="token punctuation">)</span>             <span class="token comment"># loss.backward()函数的作用是根据loss来计算网络参数的梯度，其对应的输入默认为网络的叶子节点</span>             loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>             <span class="token comment"># 所有的optimizer都实现了step()方法，这个方法会更新所有的参数</span>             optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>             <span class="token comment"># 将真实类别转化为数值型</span>             y <span class="token operator">=</span> y<span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span>             <span class="token comment"># 计算损失的总和，pytorch中的.item()用于将一个零维张量转换成浮点数。可以减少gpu内存消耗</span>             train_l_sum <span class="token operator">+=</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span>             <span class="token comment"># 如果真实label和预测的y_hat相同，则计数正确一个，以此计算训练集准确率</span>             train_acc_sum <span class="token operator">+=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>argmax<span class="token punctuation">(</span>y_hat<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">)</span> <span class="token operator">==</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span>detach<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">float</span><span class="token punctuation">(</span><span class="token punctuation">)</span>             n <span class="token operator">+=</span> <span class="token builtin">list</span><span class="token punctuation">(</span>y<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>         <span class="token comment"># 通过评估函数计算测试集的准确率</span>         test_acc <span class="token operator">=</span> evaluate_accuracy<span class="token punctuation">(</span>test_iter<span class="token punctuation">,</span> net<span class="token punctuation">)</span>           <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'epoch %d, loss %.4f, train acc %.3f, test acc %.3f'</span>\             <span class="token operator">%</span> <span class="token punctuation">(</span>epoch <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">,</span> train_l_sum <span class="token operator">/</span> n<span class="token punctuation">,</span> train_acc_sum <span class="token operator">/</span> n<span class="token punctuation">,</span> test_acc<span class="token punctuation">)</span><span class="token punctuation">)</span> </code></pre> <pre><code class="prism language-python">num_epochs<span class="token punctuation">,</span> lr<span class="token punctuation">,</span> batch_size <span class="token operator">=</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">0.5</span><span class="token punctuation">,</span> <span class="token number">256</span> train_iter<span class="token punctuation">,</span> test_iter <span class="token operator">=</span> load_data_fashion_mnist<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span> criterion <span class="token operator">=</span> nn<span class="token punctuation">.</span>CrossEntropyLoss<span class="token punctuation">(</span><span class="token punctuation">)</span> train_ch3<span class="token punctuation">(</span>net<span class="token punctuation">,</span> train_iter<span class="token punctuation">,</span> test_iter<span class="token punctuation">,</span> criterion<span class="token punctuation">,</span> num_epochs<span class="token punctuation">,</span> batch_size<span class="token punctuation">,</span> lr<span class="token punctuation">)</span> </code></pre> <pre><code>epoch 1, loss 0.0036, train acc 0.659, test acc 0.770 epoch 2, loss 0.0021, train acc 0.803, test acc 0.801 epoch 3, loss 0.0018, train acc 0.831, test acc 0.781 epoch 4, loss 0.0017, train acc 0.845, test acc 0.837 epoch 5, loss 0.0016, train acc 0.851, test acc 0.812 epoch 6, loss 0.0015, train acc 0.860, test acc 0.846 epoch 7, loss 0.0014, train acc 0.864, test acc 0.843 epoch 8, loss 0.0014, train acc 0.869, test acc 0.851 epoch 9, loss 0.0014, train acc 0.873, test acc 0.861 epoch 10, loss 0.0013, train acc 0.875, test acc 0.853 </code></pre> <h2> <a id="_273" rel="nofollow"></a>八、摘要</h2> <p>1、除了控制维数和权重向量的大小之外，dropout是另一个避免过度拟合的工具。通常情况下，这三者是联合使用的。</p> <p>2、Dropout用随机变量ℎ′代替激活ℎ，其期望值为ℎ，方差由dropout概率????给出。<br /> 辍学只在训练期间使用。</p> <p>3、经过交叉验证，隐含节点dropout率等于0.5的时候效果最好，因为这时候dropout随机生成的网络结构最多。</p> <p>4、dropout也可以被用作一种添加噪声的方法，直接对input进行操作，输入层设为更接近1的数，使得输入变化不会太大，一般选0.8。</p> <p>5、超参数的采样概率一般选1。</p> <p>6、dropout常用于提升模型泛化能力</p> <p>7、dropout也常用于解决模型费时的问题。做完dropout，相当于从原始的网络中找到一个更瘦的网络。因而，对于一个有N个节点的神经网络，有了dropout后，就可以看做是2^n个模型的集合了，但此时要训练的参数数目却是不变的，这就解决了费时的问题。</p> <h2> <a id="_292" rel="nofollow"></a>九、练习</h2> <p>1、试试如果你改变第1层和第2层的辍学概率会发生什么。特别是，如果你改变两层的概率会发生什么？</p> <p>2、增加历时的数量，比较使用放弃和不使用放弃时的结果。</p> <p>3、计算应用dropout后的激活随机变量的方差。</p> <p>4、为什么你通常不使用dropout？</p> <p>5、如果对模型进行修改，使其更加复杂，如增加隐藏层单元，使用dropout来应对过拟合的效果是否会更加明显？</p> <p>6、以本节中的模型为例，比较使用dropout和权重衰减的效果。如果同时使用dropout和权重衰减会怎样？</p> <p>7、如果我们对权重矩阵中的单个权重而不是激活应用dropout会怎样？</p> <p>8、用一个随机变量来代替dropout激活，该变量的值为[0,????/2,????] 。你能设计出比二进制dropout函数效果更好的东西吗？你为什么要使用它？为什么不呢？</p> </p></div> 			                <div class="clearfix"></div>
                <div class="col-md-12 mt-5">
                                        <p>上一个：<a href="/news/article-67282.htm">猫咪疫苗三针要隔多少天打（猫的疫苗三针隔多久）</a></p>
                                        <p>下一个：<a href="/news/article-67921.htm">哈尔滨农大动物医院地址电话是多少（哈尔滨农大兽医院）</a></p>
                                    </div>
                                </div>
                <div class="col-md-3">
                    <div class="panel panel-default">
    <div class="panel-heading">
        <h3 class="panel-title">热门文章</h3>
    </div>
    <div class="panel-body">
        <ul class="p-0 x-0" style="list-style: none;margin: 0;padding: 0;">
                        <li class="py-2"><a href="/news/article-62957.htm" title="动物疫苗接种注意问题及措施 动物疫苗接种注意问题及措施有哪些">动物疫苗接种注意问题及措施 动物疫苗接种注意问题及措施有哪些</a></li>
                        <li class="py-2"><a href="/news/article-54375.htm" title="蔬菜贝壳面(彩色贝壳做法)">蔬菜贝壳面(彩色贝壳做法)</a></li>
                        <li class="py-2"><a href="/news/article-49596.htm" title="正则表达式匹配IP的表达式">正则表达式匹配IP的表达式</a></li>
                        <li class="py-2"><a href="/news/article-46628.htm" title="适合养猫的生肖（适合养猫的生肖有哪些）">适合养猫的生肖（适合养猫的生肖有哪些）</a></li>
                        <li class="py-2"><a href="/news/article-48086.htm" title="Rancher上部署Redis部署">Rancher上部署Redis部署</a></li>
                        <li class="py-2"><a href="/news/article-45623.htm" title="58同城宠物狗狗出售怎么发布广告（怎样在58同城发布出售狗狗信息）">58同城宠物狗狗出售怎么发布广告（怎样在58同城发布出售狗狗信息）</a></li>
                        <li class="py-2"><a href="/news/article-61753.htm" title="笔记：Linux命令（目录和文件管理）">笔记：Linux命令（目录和文件管理）</a></li>
                        <li class="py-2"><a href="/free-nodes/2025-3-6-free-nekoray.htm" title="「3月6日」最高速度22.5M/S，2025年NekoRay每天更新免费节点订阅链接">「3月6日」最高速度22.5M/S，2025年NekoRay每天更新免费节点订阅链接</a></li>
                        <li class="py-2"><a href="/free-nodes/2025-1-22-node-share.htm" title="「1月22日」最高速度21.4M/S，2025年NekoRay每天更新免费节点订阅链接">「1月22日」最高速度21.4M/S，2025年NekoRay每天更新免费节点订阅链接</a></li>
                        <li class="py-2"><a href="/free-nodes/2025-1-21-linux-nekoray-node.htm" title="「1月21日」最高速度22.2M/S，2025年NekoRay每天更新免费节点订阅链接">「1月21日」最高速度22.2M/S，2025年NekoRay每天更新免费节点订阅链接</a></li>
                    </ul>
    </div>
</div>

<div class="panel panel-default">
    <div class="panel-heading">
        <h3 class="panel-title">归纳</h3>
    </div>
    <div class="panel-body">
        <ul class="p-0 x-0" style="list-style: none;margin: 0;padding: 0;">
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">21</span> <a href="/date/2025-03/" title="2025-03 归档">2025-03</a></h4>
            </li>
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">84</span> <a href="/date/2025-02/" title="2025-02 归档">2025-02</a></h4>
            </li>
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">84</span> <a href="/date/2025-01/" title="2025-01 归档">2025-01</a></h4>
            </li>
                    </ul>
    </div>
</div>

                </div>
            </div>
        </div>
    </div>
    <!-- About End -->
        <!-- Copyright Start -->
    <div class="container-fluid copyright py-4">
        <div class="container">
            <div class="row g-4 align-items-center">
                <div class="col-md-6 text-center text-md-start mb-md-0">
                            <p>
                                <a href="/">首页</a> | 
                                <a href="/free-node/">免费节点</a> | 
                                <a href="/news/">新闻资讯</a> |
                                <a href="/about-us.htm">关于我们</a> |
                                <a href="/disclaimer.htm">免责申明</a> |
                                <a href="/privacy.htm">隐私申明</a> |
                                <a href="/sitemap.xml">网站地图</a>
                            </p>
                    <span class="text-white">NekoRayNode节点订阅站 版权所有 Powered by WordPress</span>
                </div>
            </div>
        </div>
    </div>
    <!-- Copyright End -->
    <!-- Back to Top -->
    <a href="#" class="btn btn-primary btn-lg-square back-to-top"><i class="fa fa-arrow-up"></i></a>
    <!-- JavaScript Libraries -->
    <script src="https://cdn.bootcdn.net/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
    <script src="/assets/website/js/frontend/nekoraynode/easing/easing.min.js"></script>
    <script src="/assets/website/js/frontend/nekoraynode/lightbox/js/lightbox.min.js"></script>
    <!-- Template Javascript -->
    <script src="/assets/website/js/frontend/nekoraynode/main.js"></script>
    <script src="https://www.freeclashnode.com/assets/js/frontend/invite-url.js"></script><script src="/assets/website/js/frontend/G.js"></script>
</body>

</html>